from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
import json
from datasets import Dataset, DatasetDict, load_dataset
import wandb
import torch
# Before starting training, clear any cached memory
torch.cuda.empty_cache()

# If necessary, adjust memory management settings
torch.backends.cudnn.benchmark = True

def load_custom_dataset(file_path, tokenizer, split_ratio=0.9):
    with open(file_path, 'r') as file:
        data = json.load(file)
    prompts = [x['prompt'] for x in data]
    answers = [x['response']['answer'] for x in data]

    # Tokenize the data
    model_inputs = tokenizer(prompts, padding="max_length", truncation=True, max_length=512)
    labels = tokenizer(answers, padding="max_length", truncation=True, max_length=512).input_ids

    # Replace padding token ID in labels with -100
    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in label_list] for label_list in labels]

    # Creating a Dataset from these lists
    dataset = Dataset.from_dict({"input_ids": model_inputs['input_ids'], "attention_mask": model_inputs['attention_mask'], "labels": labels})
    train_test_split = dataset.train_test_split(test_size=1.0 - split_ratio)
    return DatasetDict({'train': train_test_split['train'], 'validation': train_test_split['test']})

model_checkpoint = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Ensure that tokenizer's pad token is correctly set
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
    model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to match tokenizer
else:
    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

dataset = load_custom_dataset('combined_output.json', tokenizer)

training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    num_train_epochs=3,
    gradient_accumulation_steps=16,
    fp16=True,
    save_strategy="epoch",
    logging_dir='./logs',
    report_to="wandb"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['validation'],
    tokenizer=tokenizer,
)

trainer.train()

model.save_pretrained('./finetuned_llama')
tokenizer.save_pretrained('./finetuned_llama')

print("Fine-tuning complete. Model saved to './finetuned_llama'")
wandb.finish()
